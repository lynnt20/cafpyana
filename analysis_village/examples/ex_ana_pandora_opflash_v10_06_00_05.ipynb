{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In this example, we will study output data frame from pandora.py configuration\n",
    "#### 1. Opening each data frame and check structure\n",
    "#### 2. Collect POT and scale factor to the target POT\n",
    "#### 3. Merge evtdf and mcnudf for further study\n",
    "#### 4. Draw some plots for each slice and for each pfp\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import uproot as uproot\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib import ticker\n",
    "from matplotlib.ticker import (AutoMinorLocator, MultipleLocator)\n",
    "from matplotlib import gridspec\n",
    "\n",
    "# Add the head direcoty to sys.path\n",
    "workspace_root = os.getcwd()  \n",
    "sys.path.insert(0, workspace_root + \"/../../\")\n",
    "\n",
    "# import this repo's classes\n",
    "import pyanalib.pandas_helpers as ph\n",
    "import pyanalib.split_df_helpers as splh\n",
    "import pyanalib.stat_helpers as sh\n",
    "\n",
    "np.seterr(divide='ignore', invalid='ignore', over='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_str = \"v10_06_00_05\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## df files to open\n",
    "mc_bnb_cosmic_file = \"../../mc_MCP2025B_1e20_05_prodgenie_corsika_proton_rockbox_sbnd_CV_caf_flat_caf_sbnd.df\"\n",
    "mc_rockbox_th1to100_file = \"../../mc_MCP2025B_prodgenie_corsika_proton_rockbox_lowenergydirt_sbnd_CV_caf_flat_caf_sbnd.df\"\n",
    "data_offbeam_light_file = \"../../data_MCP2025B_02_InTimeCosmics_offbeamlight_v10_06_00_05_flatcaf_sbnd.df\"\n",
    "data_bnb_light_dev_file = \"../../data_MCP2025B_05_DevSample_bnblight_v10_06_00_05_flatcaf_sbnd.df\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check keys in each file\n",
    "print(\"keys in mc_bnb_cosmic_file\")\n",
    "splh.print_keys(mc_bnb_cosmic_file)\n",
    "\n",
    "print(\"keys in data_bnb_light_dev_file\")\n",
    "splh.print_keys(data_bnb_light_dev_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check split multiplicity\n",
    "print(\"mc_bnb_cosmic_file n_split: %d\" %splh.get_n_split(mc_bnb_cosmic_file))\n",
    "print(\"data_bnb_light_dev_file n_split: %d\" %splh.get_n_split(data_bnb_light_dev_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define keys to load\n",
    "n_max_concat = 2 ## for big files, each key would have more than one split\n",
    "keys2load = ['opflash', 'hdr', 'pot', 'evt', 'mcnu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load DataFrames\n",
    "mc_bnb_cosmic_dfs = splh.load_dfs(mc_bnb_cosmic_file, keys2load, n_max_concat)\n",
    "mc_rockbox_th1to100_dfs = splh.load_dfs(mc_rockbox_th1to100_file, keys2load, n_max_concat)\n",
    "data_offbeam_light_dfs = splh.load_dfs(data_offbeam_light_file, keys2load, n_max_concat)\n",
    "data_bnb_light_dfs = splh.load_dfs(data_bnb_light_dev_file, keys2load, n_max_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_bnb_cosmic_dfs['evt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_bnb_cosmic_dfs['evt'].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check duplicated (run, subrun, evt) and filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, there are dulpications in (run, subrun, evt). This section will validate it and filter out if there is any duplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions for duplication validation and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicate_run_evt_combinations(df):\n",
    "    \"\"\"\n",
    "    Given a DataFrame with a MultiIndex of (__ntuple, entry), find all rows\n",
    "    where (run, evt) combinations are duplicated.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame with MultiIndex and columns 'run' and 'evt'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame containing only duplicated (run, evt) combinations,\n",
    "                      including the original (__ntuple, entry) indices.\n",
    "    \"\"\"\n",
    "    # Reset index to access MultiIndex as columns\n",
    "    df_reset = df.reset_index()\n",
    "\n",
    "    # Find duplicated (run, evt) combinations\n",
    "    dup_mask = df_reset.duplicated(subset=['run', 'subrun', 'evt'], keep=False)\n",
    "\n",
    "    # Extract duplicates\n",
    "    duplicates = df_reset[dup_mask]\n",
    "\n",
    "    # Sort for readability\n",
    "    duplicates_sorted = duplicates.sort_values(by=['run', 'subrun', 'evt'])\n",
    "\n",
    "    #return duplicates_sorted[['__ntuple', 'entry', 'run', 'subrun', 'evt']]\n",
    "\n",
    "    print(duplicates_sorted[['__ntuple', 'entry', 'run', 'subrun', 'evt']])\n",
    "    #return duplicates_sorted[['__ntuple', 'entry', 'run', 'subrun', 'evt']]\n",
    "\n",
    "def plot_duplicate_run_subrun_evt_distribution(df, title=\"\"):\n",
    "    \"\"\"\n",
    "    Computes and plots the distribution of how many times each (run, subrun, evt) combination appears.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame with MultiIndex (__ntuple, entry)\n",
    "                           and columns including 'run', 'subrun', and 'evt'.\n",
    "        title (str): Title for the plot (optional).\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: Duplication count distribution \n",
    "                   (index = count, value = number of (run, subrun, evt) with that count)\n",
    "    \"\"\"\n",
    "    # Reset index to work with columns\n",
    "    df_reset = df.reset_index()\n",
    "\n",
    "    # Count occurrences of each (run, subrun, evt)\n",
    "    combo_counts = df_reset.groupby(['run', 'subrun', 'evt']).size()\n",
    "\n",
    "    # Get distribution of these counts\n",
    "    distribution = combo_counts.value_counts().sort_index()\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    distribution.plot(kind='bar', log=True)\n",
    "    plt.xlabel('Number of times (run, subrun, evt) appears')\n",
    "    plt.ylabel('Entries (spills)')\n",
    "    plt.title(title)\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def filter_unique_events(df):\n",
    "    # If your index includes '__ntuple' and 'entry', reset to access them as columns\n",
    "    df_reset = df.reset_index()\n",
    "\n",
    "    # Find duplicated (run, subrun, evt) â€” keep=False marks *all* occurrences as duplicates\n",
    "    dup_mask = df_reset.duplicated(subset=['run', 'subrun', 'evt'], keep=False)\n",
    "\n",
    "    # Keep only rows that are NOT duplicated\n",
    "    unique_df = df_reset[~dup_mask]\n",
    "\n",
    "    # Optional: set index back if needed\n",
    "    return unique_df.set_index(df.index.names)\n",
    "\n",
    "def filter_using_hdr(df, hdrdf):\n",
    "    allowed_keys = set(hdrdf.index)\n",
    "\n",
    "    ntuples = df.index.get_level_values('__ntuple').to_numpy()\n",
    "    entries = df.index.get_level_values('entry').to_numpy()\n",
    "    keys = np.column_stack((ntuples, entries))\n",
    "\n",
    "    # Create mask using list comprehension (faster than pure zip for large data)\n",
    "    mask = [(nt, en) in allowed_keys for nt, en in keys]\n",
    "\n",
    "    return df[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform duplication validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"duplication for MC BNB + Cosmic sample\")\n",
    "find_duplicate_run_evt_combinations(mc_bnb_cosmic_dfs[\"hdr\"])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"duplication for low th. MC BNB + Cosmic sample\")\n",
    "find_duplicate_run_evt_combinations(mc_rockbox_th1to100_dfs[\"hdr\"])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"duplication for Off-beam + light sample\")\n",
    "find_duplicate_run_evt_combinations(data_offbeam_light_dfs[\"hdr\"])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"duplication for BNB + light sample\")\n",
    "find_duplicate_run_evt_combinations(data_bnb_light_dfs[\"hdr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_duplicate_run_subrun_evt_distribution(mc_bnb_cosmic_dfs[\"hdr\"], \"BNB + Cosmic MC\")\n",
    "plot_duplicate_run_subrun_evt_distribution(mc_rockbox_th1to100_dfs[\"hdr\"], \"Low Th. BNB + Cosmic MC\")\n",
    "plot_duplicate_run_subrun_evt_distribution(data_offbeam_light_dfs[\"hdr\"], \"Off-beam + Light Data\")\n",
    "plot_duplicate_run_subrun_evt_distribution(data_bnb_light_dfs[\"hdr\"], \"BNB + Light Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter the hdr DataFrame first, then filter other DataFrames by matching with the hdr DataFrame\n",
    "mc_bnb_cosmic_dfs[\"hdr\"] = filter_unique_events(mc_bnb_cosmic_dfs[\"hdr\"])\n",
    "mc_rockbox_th1to100_dfs[\"hdr\"] = filter_unique_events(mc_rockbox_th1to100_dfs[\"hdr\"])\n",
    "data_offbeam_light_dfs[\"hdr\"] = filter_unique_events(data_offbeam_light_dfs[\"hdr\"])\n",
    "data_bnb_light_dfs[\"hdr\"] = filter_unique_events(data_bnb_light_dfs[\"hdr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Double check if it is filtered\n",
    "find_duplicate_run_evt_combinations(mc_bnb_cosmic_dfs[\"hdr\"])\n",
    "find_duplicate_run_evt_combinations(mc_rockbox_th1to100_dfs[\"hdr\"])\n",
    "find_duplicate_run_evt_combinations(data_offbeam_light_dfs[\"hdr\"])\n",
    "find_duplicate_run_evt_combinations(data_bnb_light_dfs[\"hdr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_bnb_cosmic_dfs[\"evt\"] = filter_using_hdr(mc_bnb_cosmic_dfs[\"evt\"], mc_bnb_cosmic_dfs[\"hdr\"])\n",
    "mc_bnb_cosmic_dfs[\"opflash\"] = filter_using_hdr(mc_bnb_cosmic_dfs[\"opflash\"], mc_bnb_cosmic_dfs[\"hdr\"])\n",
    "mc_bnb_cosmic_dfs[\"pot\"] = filter_using_hdr(mc_bnb_cosmic_dfs[\"pot\"], mc_bnb_cosmic_dfs[\"hdr\"])\n",
    "mc_bnb_cosmic_dfs[\"mcnu\"] = filter_using_hdr(mc_bnb_cosmic_dfs[\"mcnu\"], mc_bnb_cosmic_dfs[\"hdr\"])\n",
    "\n",
    "mc_rockbox_th1to100_dfs[\"evt\"] = filter_using_hdr(mc_rockbox_th1to100_dfs[\"evt\"], mc_rockbox_th1to100_dfs[\"hdr\"])\n",
    "mc_rockbox_th1to100_dfs[\"opflash\"] = filter_using_hdr(mc_rockbox_th1to100_dfs[\"opflash\"], mc_rockbox_th1to100_dfs[\"hdr\"])\n",
    "mc_rockbox_th1to100_dfs[\"pot\"] = filter_using_hdr(mc_rockbox_th1to100_dfs[\"pot\"], mc_rockbox_th1to100_dfs[\"hdr\"])\n",
    "mc_rockbox_th1to100_dfs[\"mcnu\"] = filter_using_hdr(mc_rockbox_th1to100_dfs[\"mcnu\"], mc_rockbox_th1to100_dfs[\"hdr\"])\n",
    "\n",
    "data_offbeam_light_dfs[\"evt\"] = filter_using_hdr(data_offbeam_light_dfs[\"evt\"], data_offbeam_light_dfs[\"hdr\"])\n",
    "data_offbeam_light_dfs[\"opflash\"] = filter_using_hdr(data_offbeam_light_dfs[\"opflash\"], data_offbeam_light_dfs[\"hdr\"])\n",
    "data_offbeam_light_dfs[\"pot\"] = filter_using_hdr(data_offbeam_light_dfs[\"pot\"], data_offbeam_light_dfs[\"hdr\"])\n",
    "\n",
    "data_bnb_light_dfs[\"evt\"] = filter_using_hdr(data_bnb_light_dfs[\"evt\"], data_bnb_light_dfs[\"hdr\"])\n",
    "data_bnb_light_dfs[\"opflash\"] = filter_using_hdr(data_bnb_light_dfs[\"opflash\"], data_bnb_light_dfs[\"hdr\"])\n",
    "data_bnb_light_dfs[\"pot\"] = filter_using_hdr(data_bnb_light_dfs[\"pot\"], data_bnb_light_dfs[\"hdr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_evt(df):\n",
    "    unique_count = df.index.droplevel(\n",
    "        list(df.index.names[2:])  # drop everything except first two levels\n",
    "    ).nunique()\n",
    "    return unique_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Collect the offbeam data fudge factor and scale for offbeam data\n",
    "n_record_spill_data = get_n_evt(data_bnb_light_dfs['hdr'])\n",
    "n_gates_data = len(data_bnb_light_dfs[\"pot\"])\n",
    "\n",
    "n_record_spill_offbeam_data = get_n_evt(data_offbeam_light_dfs['hdr'])\n",
    "n_gates_offbeam_data = data_offbeam_light_dfs[\"hdr\"][data_offbeam_light_dfs[\"hdr\"]['first_in_subrun'] == 1]['noffbeambnb'].sum()\n",
    "\n",
    "p_trig_data = n_record_spill_data / n_gates_data\n",
    "p_trig_offbeam_data = n_record_spill_offbeam_data / n_gates_offbeam_data\n",
    "\n",
    "f_factor = (p_trig_data - p_trig_offbeam_data) / (1 - p_trig_offbeam_data)\n",
    "print(\"f_factor: %f\" %f_factor)\n",
    "\n",
    "intime_gate_scale = (1. - f_factor) * (n_gates_data + 0.) / (n_gates_offbeam_data + 0.)\n",
    "print(\"intime_gate_scale: %f\" %intime_gate_scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Collect pot scale for MC\n",
    "mc_tot_pot = mc_bnb_cosmic_dfs[\"hdr\"]['pot'].sum()\n",
    "mc_low_th_tot_pot = mc_rockbox_th1to100_dfs[\"hdr\"]['pot'].sum()\n",
    "\n",
    "data_tot_pot = data_bnb_light_dfs[\"hdr\"]['pot'].sum()\n",
    "data_tot_TOR860 = data_bnb_light_dfs[\"pot\"]['TOR860'].sum()\n",
    "data_tot_TOR875 = data_bnb_light_dfs[\"pot\"]['TOR875'].sum()\n",
    "\n",
    "print(\"mc_tot_pot: %e\" %(mc_tot_pot))\n",
    "print(\"mc_low_thtot_pot: %e\" %(mc_low_th_tot_pot))\n",
    "\n",
    "print(\"data_tot_pot: %e\" %(data_tot_pot))\n",
    "print(\"data_tot_TOR860: %e\" %(data_tot_TOR860))\n",
    "print(\"data_tot_TOR875: %e\" %(data_tot_TOR875))\n",
    "\n",
    "target_pot = data_tot_pot\n",
    "mc_pot_scale = target_pot / mc_tot_pot\n",
    "mc_low_th_scale = target_pot / mc_low_th_tot_pot\n",
    "print(\"MC POT scale: %.3f\" %(mc_pot_scale))\n",
    "print(\"MC Low Th. POT scale: %.3f\" %(mc_low_th_scale))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparison between observed and expected total number of recorded spills\n",
    "n_evt_mc = get_n_evt(mc_bnb_cosmic_dfs[\"hdr\"])\n",
    "n_evt_mc_low_th = get_n_evt(mc_rockbox_th1to100_dfs[\"hdr\"])\n",
    "\n",
    "print(\"n_evt_data_onbeam: %d\" %n_record_spill_data)\n",
    "print(\"n_evt_exp.: %f\" %(n_evt_mc * mc_pot_scale + n_evt_mc_low_th * mc_low_th_scale +n_record_spill_offbeam_data * intime_gate_scale))\n",
    "print(\"- n_evt_mc: %f\" %(n_evt_mc * mc_pot_scale))\n",
    "print(\"- n_evt_mc_low_th: %f\" %(n_evt_mc_low_th * mc_low_th_scale))\n",
    "print(\"- n_evt_data_offbeam: %f\" %(n_record_spill_offbeam_data * intime_gate_scale))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing OpFlash DataFrame and Collect PE cut Decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check OpFlash distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_a_distribution(df, col, title_x, x_min, x_max, nbins, title_y = \"\", is_logx = False, is_logy = False, label_top = \"\"):\n",
    "    var = df[col]\n",
    "    bins = np.logspace(np.log10(x_min), np.log10(x_max), nbins + 1) if is_logx else np.linspace(x_min, x_max, nbins + 1)\n",
    "    bin_centers = np.sqrt(bins[:-1] * bins[1:]) if is_logx else 0.5 * (bins[:-1] + bins[1:])\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.hist(var, bins, weights=[np.ones_like(data) * 1. for data in var], histtype=\"step\", label=[\"all\"])\n",
    "    if is_logy:\n",
    "        ax.yscale('log')\n",
    "    ax.set_xlabel(title_x)\n",
    "    ax.set_ylabel(title_y)\n",
    "    ax.legend()\n",
    "    ax.text(1.0, 1.02, label_top, transform=ax.transAxes, fontsize=14, fontweight='bold', ha='right') \n",
    "    plt.show()\n",
    "\n",
    "def abs_comp_total_pe_two_mc_stack(\n",
    "    df_mc, df_mc_low_th, df_data_bnb_light, df_data_offbeam_light,\n",
    "    col_mc, col_data, title_x, x_min, x_max, nbins,\n",
    "    title_y = \"\", is_logx = False, is_logy = False, label_top = \"\",\n",
    "    mc_scale = 1.533, low_th_mc_scale = 1., intime_scale = 1.\n",
    "):\n",
    "    mc_var = df_mc[col_mc]\n",
    "    mc_low_th_var = df_mc_low_th[col_mc]\n",
    "    data_bnb_light_var = df_data_bnb_light[col_data]\n",
    "    data_offbeam_light_var = df_data_offbeam_light[col_data]\n",
    "\n",
    "    # (kept for reference; override via intime_scale argument)\n",
    "    data_offbeam_light_scale = 0.103\n",
    "\n",
    "    # Binning\n",
    "    bins = (np.logspace(np.log10(x_min), np.log10(x_max), nbins + 1)\n",
    "            if is_logx else np.linspace(x_min, x_max, nbins + 1))\n",
    "    bin_centers = (np.sqrt(bins[:-1] * bins[1:])\n",
    "                   if is_logx else 0.5 * (bins[:-1] + bins[1:]))\n",
    "    bin_widths = np.diff(bins)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "    # --- Data histogram (BNB light) ---\n",
    "    ax.hist(\n",
    "        data_bnb_light_var, bins=bins,\n",
    "        histtype=\"step\", color=\"black\", linewidth=2,\n",
    "        label=\"BNB + light data\"\n",
    "    )\n",
    "\n",
    "    # --- Components: compute binned counts ---\n",
    "    mc_hist, _ = np.histogram(mc_var, bins=bins)\n",
    "    mc_low_th_hist, _ = np.histogram(mc_low_th_var, bins=bins)\n",
    "    offbeam_hist, _ = np.histogram(data_offbeam_light_var, bins=bins)\n",
    "\n",
    "    # Apply scales\n",
    "    mc_hist = mc_hist * mc_scale\n",
    "    mc_low_th_hist = mc_low_th_hist * low_th_mc_scale\n",
    "    offbeam_hist = offbeam_hist * intime_scale\n",
    "\n",
    "    # --- Stack components as bars (to show contributions) ---\n",
    "    # Bottoms for stacking\n",
    "    bottom0 = np.zeros_like(mc_hist, dtype=float)\n",
    "    h1 = mc_hist\n",
    "    h2 = mc_low_th_hist\n",
    "    h3 = offbeam_hist\n",
    "\n",
    "    # Use bars so stack works in both linear and log-x; they also respect log-y.\n",
    "    # (We avoid specifying colors explicitly so you can theme globally if you like.)\n",
    "    ax.bar(bins[:-1], h1, width=bin_widths, align=\"edge\",\n",
    "           alpha=0.45, edgecolor=\"none\", label=\"MC (BNB+cosmic)\")\n",
    "    ax.bar(bins[:-1], h2, width=bin_widths, align=\"edge\",\n",
    "           bottom=h1, alpha=0.45, edgecolor=\"none\", label=\"MC low-th\")\n",
    "    ax.bar(bins[:-1], h3, width=bin_widths, align=\"edge\",\n",
    "           bottom=h1 + h2, alpha=0.45, edgecolor=\"none\", label=\"Off-beam light\")\n",
    "\n",
    "    # --- Total prediction overlay (dashed step) ---\n",
    "    pred_hist = h1 + h2 + h3\n",
    "    ax.step(bin_centers, pred_hist, where=\"mid\", linewidth=2, linestyle=\"--\",\n",
    "            label=\"Total prediction\", color='red')\n",
    "\n",
    "    # Scales, labels, cosmetics\n",
    "    if is_logx:\n",
    "        ax.set_xscale(\"log\")\n",
    "    if is_logy:\n",
    "        ax.set_yscale(\"log\")\n",
    "\n",
    "    ax.set_xlabel(title_x)\n",
    "    ax.set_ylabel(title_y if title_y else \"Events\")\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "\n",
    "    # Legend: put data first, then components, then total\n",
    "    # (Matplotlib collects in order added; this already matches that.)\n",
    "    ax.legend(frameon=False)\n",
    "\n",
    "    if label_top:\n",
    "        ax.text(1.0, 1.02, label_top, transform=ax.transAxes,\n",
    "                fontsize=14, fontweight='bold', ha='right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_bnb_cosmic_opflash_df = mc_bnb_cosmic_dfs['opflash']\n",
    "mc_rockbox_th1to100_df = mc_rockbox_th1to100_dfs['opflash']\n",
    "data_bnb_light_opflash_df = data_bnb_light_dfs['opflash']\n",
    "data_offbeam_light_opflash_df = data_offbeam_light_dfs['opflash']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check OpFlash First Time distributions\n",
    "draw_a_distribution(mc_bnb_cosmic_opflash_df, ('firsttime'), \"First Time [us]\", -15., 25., 100, \"OpFlashes\", False, False, \"MC BNB + Cosmic\")\n",
    "draw_a_distribution(mc_rockbox_th1to100_df, ('firsttime'), \"First Time [us]\", -15., 25., 100, \"OpFlashes\", False, False, \"MC BNB + Cosmic, Rockbox Th. 1 - 100 MeV\")\n",
    "draw_a_distribution(data_offbeam_light_opflash_df, ('firsttime'), \"First Time [us]\", -15., 25., 100, \"OpFlashes\", False, False, \"Data Off-beam + Light\")\n",
    "draw_a_distribution(data_bnb_light_opflash_df, ('firsttime'), \"First Time [us]\", -15., 25., 100, \"OpFlashes\", False, False, \"Data BNB + Light\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Based on the previous block's result, we use [-5 us, +5 us] window for collecting OpFlashes corresponding to the trigger\n",
    "\n",
    "mc_bnb_light_opflash_time_cut_df = mc_bnb_cosmic_opflash_df[(mc_bnb_cosmic_opflash_df.firsttime > -5.) & (mc_bnb_cosmic_opflash_df.firsttime < 5.)]\n",
    "mc_rockbox_th1to100_opflash_time_cut_df = mc_rockbox_th1to100_df[(mc_rockbox_th1to100_df.firsttime > -5.) & (mc_rockbox_th1to100_df.firsttime < 5.)]\n",
    "data_offbeam_light_opflash_df = data_offbeam_light_opflash_df[(data_offbeam_light_opflash_df.firsttime > -5.) & (data_offbeam_light_opflash_df.firsttime < 5.)]\n",
    "data_bnb_light_opflash_df = data_bnb_light_opflash_df[(data_bnb_light_opflash_df.firsttime > -5.) & (data_bnb_light_opflash_df.firsttime < 5.)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sum PEs for OpFlashes within the timing window\n",
    "\n",
    "mc_bnb_cosmic_totalpe_sum_time_cut=(mc_bnb_light_opflash_time_cut_df.groupby(level=[0, 1])[\"totalpe\"].sum())\n",
    "mc_rockbox_th1to100_totalpe_sum_time_cut=(mc_rockbox_th1to100_opflash_time_cut_df.groupby(level=[0, 1])[\"totalpe\"].sum())\n",
    "data_offbeam_light_totalpe_sum_time_cut=(data_offbeam_light_opflash_df.groupby(level=[0, 1])[\"totalpe\"].sum())\n",
    "data_bnb_light_totalpe_sum_time_cut=(data_bnb_light_opflash_df.groupby(level=[0, 1])[\"totalpe\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add the total PE column to the hdr DataFrames, and fill empty (NaN) entries with 0.\n",
    "\n",
    "mc_bnb_cosmic_dfs['hdr']['totalpe_timecut'] = mc_bnb_cosmic_totalpe_sum_time_cut\n",
    "mc_bnb_cosmic_dfs['hdr']['totalpe_timecut'] = (mc_bnb_cosmic_dfs['hdr']['totalpe_timecut'].fillna(-1.))\n",
    "\n",
    "mc_rockbox_th1to100_dfs['hdr']['totalpe_timecut'] = mc_rockbox_th1to100_totalpe_sum_time_cut\n",
    "mc_rockbox_th1to100_dfs['hdr']['totalpe_timecut'] = (mc_rockbox_th1to100_dfs['hdr']['totalpe_timecut'].fillna(-1.))\n",
    "\n",
    "data_offbeam_light_dfs['hdr']['totalpe_timecut'] = data_offbeam_light_totalpe_sum_time_cut\n",
    "data_offbeam_light_dfs['hdr']['totalpe_timecut'] = (data_offbeam_light_dfs['hdr']['totalpe_timecut'].fillna(-1.))\n",
    "\n",
    "data_bnb_light_dfs['hdr']['totalpe_timecut'] = data_bnb_light_totalpe_sum_time_cut\n",
    "data_bnb_light_dfs['hdr']['totalpe_timecut'] = (data_bnb_light_dfs['hdr']['totalpe_timecut'].fillna(-1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the total PE distributions\n",
    "\n",
    "draw_a_distribution(mc_bnb_cosmic_dfs['hdr'], ('totalpe_timecut'), \"Total PE\", -10, 50000, 1000, \"Entries\", False, False, \"MC BNB + Cosmic\")\n",
    "draw_a_distribution(mc_rockbox_th1to100_dfs['hdr'], ('totalpe_timecut'), \"Total PE\", -10, 50000, 1000, \"Entries\", False, False, \"MC BNB + Cosmic, Rockbox Th. 1 - 100 MeV\")\n",
    "draw_a_distribution(data_offbeam_light_dfs['hdr'], ('totalpe_timecut'), \"Total PE\", -10, 50000, 1000, \"Entries\", False, False, \"Data Off-beam + Light\")\n",
    "draw_a_distribution(data_bnb_light_dfs['hdr'], ('totalpe_timecut'), \"Total PE\", -10, 50000, 1000, \"Entries\", False, False, \"Data BNB + Light\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply PE scale correction to MC and collect PE cut decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply a scale to MC total PE\n",
    "mc_bnb_cosmic_hdr_df = mc_bnb_cosmic_dfs['hdr']\n",
    "mc_bnb_cosmic_hdr_df['totalpe_timecut_0p66'] = mc_bnb_cosmic_hdr_df['totalpe_timecut'] * 0.66\n",
    "\n",
    "mc_rockbox_th1to100_hdr_df = mc_rockbox_th1to100_dfs['hdr']\n",
    "mc_rockbox_th1to100_hdr_df['totalpe_timecut_0p66'] = mc_rockbox_th1to100_hdr_df['totalpe_timecut'] * 0.66\n",
    "\n",
    "data_offbeam_light_hdr_df = data_offbeam_light_dfs['hdr']\n",
    "data_bnb_light_hdr_df = data_bnb_light_dfs['hdr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compared the default and scaled total PE columns\n",
    "mc_bnb_cosmic_hdr_df[[\"totalpe_timecut\", \"totalpe_timecut_0p66\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check total PE distribution\n",
    "abs_comp_total_pe_two_mc_stack(mc_bnb_cosmic_hdr_df, mc_rockbox_th1to100_hdr_df, data_bnb_light_hdr_df, data_offbeam_light_hdr_df, ('totalpe_timecut_0p66'), ('totalpe_timecut'), \"Total PE\", -10., 40000, 100, \"Entries\", False, False, \"MC PE scale 0.66\", mc_pot_scale, mc_low_th_scale, intime_gate_scale)\n",
    "abs_comp_total_pe_two_mc_stack(mc_bnb_cosmic_hdr_df, mc_rockbox_th1to100_hdr_df, data_bnb_light_hdr_df, data_offbeam_light_hdr_df, ('totalpe_timecut_0p66'), ('totalpe_timecut'), \"Total PE\", -100, 5000, 100, \"Entries\", False, False, \"MC PE scale 0.66\", mc_pot_scale, mc_low_th_scale, intime_gate_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEFINE PE CUT\n",
    "total_pe_cut = 2000.\n",
    "mc_bnb_cosmic_pe_mask = mc_bnb_cosmic_hdr_df[\"totalpe_timecut_0p66\"] > total_pe_cut\n",
    "mc_low_th_pe_mask = mc_rockbox_th1to100_hdr_df['totalpe_timecut_0p66'] > total_pe_cut\n",
    "data_offbeam_pe_mask = data_offbeam_light_hdr_df['totalpe_timecut'] > total_pe_cut\n",
    "data_bnb_light_pe_mask = data_bnb_light_hdr_df['totalpe_timecut'] > total_pe_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Since evt DataFrame has more level in labels, we should make a mask for considering the difference\n",
    "def get_evt_pe_mask(mask, df):\n",
    "    sel_idx = mask[mask].index\n",
    "    pairs = df.index.droplevel([-2, -1])\n",
    "    mask2 = pairs.isin(sel_idx)\n",
    "    return mask2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_bnb_cosmic_evt_pe_mask = get_evt_pe_mask(mc_bnb_cosmic_pe_mask, mc_bnb_cosmic_dfs[\"evt\"])\n",
    "mc_low_th_evt_pe_mask = get_evt_pe_mask(mc_low_th_pe_mask, mc_rockbox_th1to100_dfs[\"evt\"])\n",
    "data_offbeam_evt_pe_mask = get_evt_pe_mask(data_offbeam_pe_mask, data_offbeam_light_dfs[\"evt\"])\n",
    "data_bnb_light_evt_pe_mask = get_evt_pe_mask(data_bnb_light_pe_mask, data_bnb_light_dfs[\"evt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Double checking for legnths of evt DataFrame and the mask array\n",
    "print(\"mc_bnb_cosmic_dfs['evt'] length: %d\" %len(mc_bnb_cosmic_dfs['evt']))\n",
    "print(\"mc_bnb_cosmic_evt_pe_mask length: %d\" %len(mc_bnb_cosmic_evt_pe_mask))\n",
    "\n",
    "print(\"mc_rockbox_th1to100_dfs['evt'] length: %d\" %len(mc_rockbox_th1to100_dfs['evt']))\n",
    "print(\"mc_low_th_evt_pe_mask length: %d\" %len(mc_low_th_evt_pe_mask))\n",
    "\n",
    "print(\"data_offbeam_light_dfs['evt'] length: %d\" %len(data_offbeam_light_dfs['evt']))\n",
    "print(\"data_offbeam_evt_pe_mask length: %d\" %len(data_offbeam_evt_pe_mask))\n",
    "\n",
    "print(\"data_bnb_light_dfs['evt'] length: %d\" %len(data_bnb_light_dfs['evt']))\n",
    "print(\"data_bnb_light_evt_pe_mask length: %d\" %len(data_bnb_light_evt_pe_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add PE cut decision (boolean) column to evt DataFrame\n",
    "mc_bnb_cosmic_dfs[\"evt\"][('pemask', '', '', '', '', '')] = mc_bnb_cosmic_evt_pe_mask\n",
    "mc_rockbox_th1to100_dfs[\"evt\"][('pemask', '', '', '', '', '')] = mc_low_th_evt_pe_mask\n",
    "data_bnb_light_dfs[\"evt\"][('pemask', '', '', '', '', '')] = data_bnb_light_evt_pe_mask\n",
    "data_offbeam_light_dfs[\"evt\"][('pemask', '', '', '', '', '')] = data_offbeam_evt_pe_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Fiducial Volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InFV_trk(data): # cm\n",
    "    xmin = -190.\n",
    "    ymin = -190.\n",
    "    zmin = 10.\n",
    "    xmax = 190.\n",
    "    ymax =  190.\n",
    "    zmax =  450.\n",
    "    return (data.x > xmin) & (data.x < xmax) & (data.y > ymin) & (data.y < ymax) & (data.z > zmin) & (data.z < zmax)\n",
    "\n",
    "def InFV(data): # cm\n",
    "    xmin = -190.\n",
    "    ymin = -190.\n",
    "    zmin = 10.\n",
    "    xmax = 190.\n",
    "    ymax =  190.\n",
    "    zmax =  450.\n",
    "    return (np.abs(data.x) > 10) & (np.abs(data.x) < 190) & (data.y > ymin) & (data.y < ymax) & (data.z > zmin) & (data.z < zmax)\n",
    "\n",
    "def InFV_xy(data): # cm\n",
    "    xmin = 10.\n",
    "    ymin = -190.\n",
    "    xmax = 190.\n",
    "    ymax =  190.\n",
    "    return (np.abs(data.x) > xmin) & (np.abs(data.x) < xmax) & (data.y > ymin) & (data.y < ymax)\n",
    "\n",
    "## Vetoing high y and high z region\n",
    "def InFV_nohiyz(data):\n",
    "    xmin = 10.\n",
    "    xmax = 190.\n",
    "    zmin = 10.\n",
    "    zmax = 450.\n",
    "    ymax_highz = 100.\n",
    "    pass_xz = (np.abs(data.x) > xmin) & (np.abs(data.x) < xmax) & (data.z > zmin) & (data.z < zmax)\n",
    "    pass_y = ((data.z < 250) & (np.abs(data.y) < 190.)) | ((data.z > 250) & (data.y > -190.) & (data.y < ymax_highz))\n",
    "    return pass_xz & pass_y\n",
    "\n",
    "def InFV_nohiyz_trk(data):\n",
    "    xmax = 190.\n",
    "    zmin = 10.\n",
    "    zmax = 450.\n",
    "    ymax_highz = 100.\n",
    "    pass_xz = (np.abs(data.x) < xmax) & (data.z > zmin) & (data.z < zmax)\n",
    "    pass_y = ((data.z < 250) & (np.abs(data.y) < 190.)) | ((data.z > 250) & (data.y > -190.) & (data.y < ymax_highz))\n",
    "    return pass_xz & pass_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform matching between evt and mcnu DataFrames. Assign nuint_categ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example from numuCC coherent charged pion production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_t(df):\n",
    "    t = (df.E - df.mu.genE - df.cpi.genE)**2 - (df.momentum.x - df.mu.genp.x - df.cpi.genp.x)**2 - (df.momentum.y - df.mu.genp.y - df.cpi.genp.y)**2 - (df.momentum.z - df.mu.genp.z - df.cpi.genp.z)**2\n",
    "    return np.abs(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_t = get_true_t(mc_bnb_cosmic_dfs['mcnu']).fillna(999999)\n",
    "mc_bnb_cosmic_dfs['mcnu']['true_t'] = true_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- truth level flags\n",
    "def Signal(df): # definition\n",
    "    is_fv = InFV_nohiyz(df.position)\n",
    "    is_1pi0p = (df.nmu_27MeV == 1) & (df.npi_30MeV == 1) & (df.np_20MeV == 0) & (df.npi0 == 0) & (df.nn_0MeV == 0) #(df.true_t < 0.1)\n",
    "    return is_fv & is_1pi0p\n",
    "\n",
    "def CCCOH(df):\n",
    "    is_cc = df.iscc\n",
    "    genie_mode = df.genie_mode\n",
    "    return is_cc & (genie_mode == 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For MC BNB + cosmic sample\n",
    "is_fv = InFV_nohiyz(mc_bnb_cosmic_dfs['mcnu'].position)\n",
    "is_signal = Signal(mc_bnb_cosmic_dfs['mcnu'])\n",
    "is_cc = mc_bnb_cosmic_dfs['mcnu'].iscc\n",
    "genie_mode = mc_bnb_cosmic_dfs['mcnu'].genie_mode\n",
    "w = mc_bnb_cosmic_dfs['mcnu'].w\n",
    "\n",
    "try :\n",
    "    nuint_categ = pd.Series(8, index=mc_bnb_cosmic_dfs['mcnu'].index)\n",
    "    #print(f\"done init nuint_categ\")\n",
    "except Exception as e:\n",
    "    print(f\"Error init nuint_categ\")\n",
    "\n",
    "nuint_categ[~is_fv] = -1  # Out of FV\n",
    "nuint_categ[is_fv & is_signal] = 1 # Signal\n",
    "nuint_categ[is_fv & ~is_cc & ~is_signal] = 0  # NC\n",
    "nuint_categ[is_fv & is_cc & ~is_signal & (genie_mode == 3)] = 2  # Non-signal CCCOH\n",
    "nuint_categ[is_fv & is_cc & ~is_signal & (genie_mode == 0)] = 3  # CCQE\n",
    "nuint_categ[is_fv & is_cc & ~is_signal & (genie_mode == 10)] = 4  # 2p2h\n",
    "nuint_categ[is_fv & is_cc & ~is_signal & (genie_mode != 0) & (genie_mode != 3) & (genie_mode != 10) & ((w < 1.4) | (genie_mode == 1))] = 5  # RES\n",
    "nuint_categ[is_fv & is_cc & ~is_signal & (genie_mode != 0) & (genie_mode != 3) & (genie_mode != 10) & ((w > 2.0) | (genie_mode == 2))] = 6  # DIS\n",
    "\n",
    "mc_bnb_cosmic_dfs['mcnu']['nuint_categ'] = nuint_categ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For MC Low Th. BNB + cosmic sample\n",
    "is_fv = InFV_nohiyz(mc_rockbox_th1to100_dfs['mcnu'].position)\n",
    "is_signal = Signal(mc_rockbox_th1to100_dfs['mcnu'])\n",
    "is_cc = mc_rockbox_th1to100_dfs['mcnu'].iscc\n",
    "genie_mode = mc_rockbox_th1to100_dfs['mcnu'].genie_mode\n",
    "w = mc_rockbox_th1to100_dfs['mcnu'].w\n",
    "\n",
    "try :\n",
    "    nuint_categ = pd.Series(8, index=mc_rockbox_th1to100_dfs['mcnu'].index)\n",
    "    #print(f\"done init nuint_categ\")\n",
    "except Exception as e:\n",
    "    print(f\"Error init nuint_categ\")\n",
    "\n",
    "nuint_categ[~is_fv] = -1  # Out of FV\n",
    "nuint_categ[is_fv & is_signal] = 1 # Signal\n",
    "nuint_categ[is_fv & ~is_cc & ~is_signal] = 0  # NC\n",
    "nuint_categ[is_fv & is_cc & ~is_signal & (genie_mode == 3)] = 2  # Non-signal CCCOH\n",
    "nuint_categ[is_fv & is_cc & ~is_signal & (genie_mode == 0)] = 3  # CCQE\n",
    "nuint_categ[is_fv & is_cc & ~is_signal & (genie_mode == 10)] = 4  # 2p2h\n",
    "nuint_categ[is_fv & is_cc & ~is_signal & (genie_mode != 0) & (genie_mode != 3) & (genie_mode != 10) & ((w < 1.4) | (genie_mode == 1))] = 5  # RES\n",
    "nuint_categ[is_fv & is_cc & ~is_signal & (genie_mode != 0) & (genie_mode != 3) & (genie_mode != 10) & ((w > 2.0) | (genie_mode == 2))] = 6  # DIS\n",
    "\n",
    "mc_rockbox_th1to100_dfs['mcnu']['nuint_categ'] = nuint_categ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match for mc bnb cosmic\n",
    "mc_bnb_cosmic_dfs[\"mcnu\"].columns = pd.MultiIndex.from_tuples([('gen',) + col if isinstance(col, tuple) else ('gen', col) for col in mc_bnb_cosmic_dfs[\"mcnu\"].columns])\n",
    "mc_bnb_cosmic_dfs[\"mcnu\"].columns = pd.MultiIndex.from_tuples([\n",
    "    col + ('',) * (6 - len(col)) for col in mc_bnb_cosmic_dfs[\"mcnu\"].columns\n",
    "])\n",
    "mc_bnb_cosmic_dfs[\"evt\"] = ph.multicol_merge(mc_bnb_cosmic_dfs[\"evt\"].reset_index(), mc_bnb_cosmic_dfs[\"mcnu\"].reset_index(),\n",
    "                                            left_on=[('__ntuple', '', '', '', '', ''), ('entry', '', '', '', '', ''), ('slc','tmatch', 'idx', '', '', '')],\n",
    "                                            right_on=[('__ntuple', '', '', '', '', ''), ('entry', '', '', '', '', ''), ('rec.mc.nu..index', '','', '', '', '')], \n",
    "                                            how=\"left\") ## -- save all sllices\n",
    "mc_bnb_cosmic_dfs[\"evt\"] = mc_bnb_cosmic_dfs[\"evt\"].set_index([\"__ntuple\", \"entry\", \"rec.slc..index\", \"rec.slc.reco.pfp..index\"], verify_integrity=True)\n",
    "mc_bnb_cosmic_dfs[\"evt\"].loc[mc_bnb_cosmic_dfs[\"evt\"][('gen', 'nuint_categ', '', '', '', '')].isna(), [('gen', 'nuint_categ', '', '', '', '')]] = -2\n",
    "\n",
    "mc_rockbox_th1to100_dfs[\"mcnu\"].columns = pd.MultiIndex.from_tuples([('gen',) + col if isinstance(col, tuple) else ('gen', col) for col in mc_rockbox_th1to100_dfs[\"mcnu\"].columns])\n",
    "mc_rockbox_th1to100_dfs[\"mcnu\"].columns = pd.MultiIndex.from_tuples([\n",
    "    col + ('',) * (6 - len(col)) for col in mc_rockbox_th1to100_dfs[\"mcnu\"].columns\n",
    "])\n",
    "mc_rockbox_th1to100_dfs[\"evt\"] = ph.multicol_merge(mc_rockbox_th1to100_dfs[\"evt\"].reset_index(), mc_rockbox_th1to100_dfs[\"mcnu\"].reset_index(),\n",
    "                            left_on=[('__ntuple', '', '', '', '', ''), ('entry', '', '', '', '', ''), ('slc','tmatch', 'idx', '', '', '')],\n",
    "                            right_on=[('__ntuple', '', '', '', '', ''), ('entry', '', '', '', '', ''), ('rec.mc.nu..index', '','', '', '', '')], \n",
    "                            how=\"left\") ## -- save all sllices\n",
    "mc_rockbox_th1to100_dfs[\"evt\"] = mc_rockbox_th1to100_dfs[\"evt\"].set_index([\"__ntuple\", \"entry\", \"rec.slc..index\", \"rec.slc.reco.pfp..index\"], verify_integrity=True)\n",
    "mc_rockbox_th1to100_dfs[\"evt\"].loc[mc_rockbox_th1to100_dfs[\"evt\"][('gen', 'nuint_categ', '', '', '', '')].isna(), [('gen', 'nuint_categ', '', '', '', '')]] = -2\n",
    "mc_rockbox_th1to100_dfs[\"evt\"][('gen', 'nuint_categ', '', '', '', '')] = -4\n",
    "\n",
    "data_offbeam_light_dfs[\"evt\"][('gen', 'nuint_categ', '', '', '', '')] = -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check \n",
    "mc_bnb_cosmic_dfs[\"mcnu\"].gen.nuint_categ.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data vs MC plots for TPC variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note: the model_list and mode_labels are based on truth level event categories in the previous section\n",
    "\n",
    "mode_list = [1, 2, 0, 3, 4, 5, 6, -1, -2, -3, -4]\n",
    "mode_labels = [\"Signal\", \"Non-Sig. CCCOH\", \"NC\", \"QE\", \"2p2h\", \"RES\", \"DIS\", \"Non-FV\", \"Others\", \"Intime Cosmics\", \"Low Th. Rockbox\"]\n",
    "colors = ['#d62728',  # Red            \n",
    "          '#1f77b4',  # Blue\n",
    "          '#ff7f0e',  # Orange\n",
    "          '#2ca02c',  # Green\n",
    "          '#17becf',  # Teal\n",
    "          '#9467bd',  # Purple\n",
    "          '#8c564b',  # Brown\n",
    "          '#e377c2',  # Pink\n",
    "          '#7f7f7f',  # Gray\n",
    "          '#bcbd22',  # Yellow-green\n",
    "          '#FFD700']  # Gold\n",
    "\n",
    "def draw_reco_stacked_hist(var_mc, var_mc_low_th, var_offbeam_data, is_logx, is_logy,\n",
    "                           title_x, title_y, x_min, x_max, nbins, outname,\n",
    "                           data_overlay=False, var_data=[], draw_density=False):\n",
    "    \n",
    "    ## Define the output figure to have two pads\n",
    "    fig = plt.figure(figsize=(8, 8), dpi=100)\n",
    "    gs = gridspec.GridSpec(2, 1, height_ratios=[5, 1], hspace=0.10)\n",
    "    ax_main = fig.add_subplot(gs[0])\n",
    "    ax_ratio = fig.add_subplot(gs[1], sharex=ax_main)\n",
    "    \n",
    "    if is_logx:\n",
    "        ax_main.set_xscale('log')\n",
    "        ax_ratio.set_xscale('log')        \n",
    "    if is_logy:\n",
    "        ax_main.set_yscale('log')\n",
    "\n",
    "\n",
    "    ax_main.set_xlabel(\"\")  # Only bottom has x-label\n",
    "    ax_main.set_ylabel(title_y)\n",
    "    ax_ratio.set_ylabel(\"Data/MC\", fontsize=12)\n",
    "    ax_ratio.set_xlabel(title_x, fontsize = 20)\n",
    "\n",
    "    ax_ratio.axhline(1.0, color='red', linestyle='--', linewidth=1)\n",
    "    ax_ratio.set_ylabel(\"Data/MC\", fontsize=12)\n",
    "    ax_ratio.set_xlabel(title_x, fontsize=12)\n",
    "    ax_ratio.set_ylim(0.5, 1.5)\n",
    "    ax_ratio.tick_params(width=2, length=6)\n",
    "    for spine in ax_ratio.spines.values():\n",
    "        spine.set_linewidth(2)\n",
    "\n",
    "    plt.setp(ax_main.get_xticklabels(), visible=False)\n",
    "\n",
    "    ## Defind binning\n",
    "    if is_logx:\n",
    "        bins = np.logspace(np.log10(x_min), np.log10(x_max), nbins + 1)\n",
    "        bin_centers = np.sqrt(bins[:-1] * bins[1:])\n",
    "    else:\n",
    "        bins = np.linspace(x_min, x_max, nbins + 1)\n",
    "        bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "\n",
    "    ## Define data for MC\n",
    "    all_mc_data = var_mc + var_mc_low_th + var_offbeam_data\n",
    "    all_weights = (\n",
    "        [np.ones_like(data) * mc_pot_scale for data in var_mc] +\n",
    "        [np.ones_like(data) * mc_low_th_scale for data in var_mc_low_th] + \n",
    "        [np.ones_like(data) * intime_gate_scale for data in var_offbeam_data]\n",
    "    )\n",
    "    each_mc_hist_data = [np.histogram(data, bins=bins, weights=w)[0] for data, w in zip(all_mc_data, all_weights)]\n",
    "    total_mc = np.sum(each_mc_hist_data, axis=0)\n",
    "\n",
    "    ## Plot stacked MC\n",
    "    hist_data, bins, _ = ax_main.hist(all_mc_data,\n",
    "                                      bins=bins,\n",
    "                                      weights=all_weights,\n",
    "                                      stacked=True,\n",
    "                                      color=colors,\n",
    "                                      label=mode_labels,\n",
    "                                      edgecolor='none',\n",
    "                                      linewidth=0,\n",
    "                                      density=draw_density,\n",
    "                                      histtype='stepfilled')\n",
    "\n",
    "    max_y = np.max(total_mc)\n",
    "\n",
    "    ## Plot MC stat error box\n",
    "    each_mc_hist_data = []\n",
    "    each_mc_hist_err2 = []  # sum of squared weights for error\n",
    "\n",
    "    for data, w in zip(all_mc_data, all_weights):\n",
    "        hist_vals, _ = np.histogram(data, bins=bins, weights=w)\n",
    "        hist_err2, _ = np.histogram(data, bins=bins, weights=np.square(w))\n",
    "        each_mc_hist_data.append(hist_vals)\n",
    "        each_mc_hist_err2.append(hist_err2)\n",
    "\n",
    "    total_mc = np.sum(each_mc_hist_data, axis=0)\n",
    "    total_mc_err2 = np.sum(each_mc_hist_err2, axis=0)\n",
    "    mc_stat_err = np.sqrt(total_mc_err2)\n",
    "    #mc_stat_err = np.sqrt(total_mc)\n",
    "\n",
    "    ax_main.bar(\n",
    "       bin_centers,\n",
    "        2 * mc_stat_err,\n",
    "        width=np.diff(bins),\n",
    "        bottom=total_mc - mc_stat_err,\n",
    "        facecolor='none',             # transparent fill\n",
    "        edgecolor='black',            # outline color of the hatching\n",
    "        hatch='xxxx',                 # hatch pattern similar to ROOT's 3004\n",
    "        linewidth=0.0,\n",
    "        label='MC Stat. Unc.'\n",
    "    )\n",
    "\n",
    "    ax_main.tick_params(width=2, length=10)\n",
    "    for spine in ax_main.spines.values():\n",
    "        spine.set_linewidth(2)\n",
    "\n",
    "    ## Draw Ratio error bar\n",
    "    mc_stat_err_ratio = mc_stat_err / total_mc\n",
    "    mc_content_ratio = total_mc / total_mc\n",
    "    mc_stat_err_ratio = np.nan_to_num(mc_stat_err_ratio, nan=0.)\n",
    "    mc_content_ratio = np.nan_to_num(mc_content_ratio, nan=-999.)\n",
    "    ax_ratio.bar(\n",
    "        bin_centers,\n",
    "        2*mc_stat_err_ratio,\n",
    "        width=np.diff(bins),\n",
    "        bottom=mc_content_ratio - mc_stat_err_ratio,\n",
    "        facecolor='none',             # transparent fill\n",
    "        edgecolor='black',            # outline color of the hatching\n",
    "        hatch='xxxx',                 # hatch pattern similar to ROOT's 3004\n",
    "        linewidth=0.0,\n",
    "        label='MC Stat. Unc.'\n",
    "    )\n",
    "\n",
    "    ## Draw data too\n",
    "    if data_overlay:\n",
    "        ax_main.set_ylabel(\"Events (POT = %.2e)\" % target_pot)\n",
    "        if draw_density:\n",
    "            ax_main.set_ylabel(\"A.U.\")\n",
    "\n",
    "        ## Define data histogram\n",
    "        counts, _ = np.histogram(var_data, bins=bins)\n",
    "\n",
    "        bin_widths = np.diff(bins)\n",
    "        total_data = np.sum(counts)\n",
    "        norm_counts = counts\n",
    "        data_eylow, data_eyhigh = sh.return_data_stat_err(counts)\n",
    "\n",
    "        if draw_density:\n",
    "            norm_counts = counts / (total_data * bin_widths)\n",
    "            data_eylow = data_eylow / (total_data * bin_widths) if total_data > 0 else np.zeros_like(counts)\n",
    "            data_eyhigh = data_eyhigh / (total_data * bin_widths) if total_data > 0 else np.zeros_like(counts)\n",
    "\n",
    "        errors = data_eylow + data_eyhigh\n",
    "        \n",
    "        ## Plot data points on main histogram\n",
    "        #ax_main.errorbar(bin_centers, norm_counts, yerr=errors,\n",
    "        #                 fmt='o', color='black', label='Data',\n",
    "        #                 markersize=5, capsize=3, linewidth=1.5)\n",
    "        \n",
    "        ax_main.errorbar(bin_centers, norm_counts,\n",
    "                 yerr=np.vstack((data_eylow, data_eyhigh)),\n",
    "                 fmt='o', color='black', label='Data',\n",
    "                 markersize=5, capsize=3, linewidth=1.5)\n",
    "        \n",
    "        max_y_data = np.max(norm_counts + data_eyhigh)\n",
    "        #print(\"max_y: %f\" %(max_y))\n",
    "        #print(\"max_y_data: %f\" %(max_y_data))\n",
    "        max_y = max(max_y, max_y_data)\n",
    "        #print(\"max_y: %f\" %(max_y))\n",
    "\n",
    "        ## Make data/mc ratio plot\n",
    "        data_ratio = norm_counts / total_mc\n",
    "        data_ratio_eylow = data_eylow / total_mc\n",
    "        data_ratio_eyhigh = data_eyhigh / total_mc\n",
    "        data_ratio = np.nan_to_num(data_ratio, nan=-999.)\n",
    "        data_ratio_eylow = np.nan_to_num(data_ratio_eylow, nan=0.)\n",
    "        data_ratio_eyhigh = np.nan_to_num(data_ratio_eyhigh, nan=0.)\n",
    "        \n",
    "        #data_ratio_errors = data_ratio_eylow + data_ratio_eyhigh\n",
    "        #ax_ratio.errorbar(bin_centers, data_ratio, yerr=data_ratio_errors,\n",
    "        #                 fmt='o', color='black', label='Data',\n",
    "        #                 markersize=5, capsize=3, linewidth=1.5)\n",
    "\n",
    "        ax_ratio.errorbar(bin_centers, data_ratio,\n",
    "                  yerr=np.vstack((data_ratio_eylow, data_ratio_eyhigh)),\n",
    "                  fmt='o', color='black', label='Data',\n",
    "                  markersize=5, capsize=3, linewidth=1.5)\n",
    "\n",
    "    ## Set ax_main axes variables\n",
    "    ax_main.set_xlim(x_min, x_max)\n",
    "    ax_main.set_ylim(0.0, max_y * 1.5)\n",
    "    if is_logy:\n",
    "        ax_main.set_ylim(0.1, max_y * 600)\n",
    "    \n",
    "    # Legend with fractions\n",
    "    accum_sum = [np.sum(data) for data in hist_data]\n",
    "    accum_sum = [0.] + accum_sum\n",
    "    total_sum = accum_sum[-1]\n",
    "    individual_sums = [accum_sum[i + 1] - accum_sum[i] for i in range(len(accum_sum) - 1)]\n",
    "    fractions = [(count / total_sum) * 100 for count in individual_sums]\n",
    "    legend_labels = [f\"{label} ({frac:.1f}%)\" for label, frac in zip(mode_labels[::-1], fractions[::-1])]\n",
    "    if data_overlay:\n",
    "        if draw_density:\n",
    "            legend_labels.append(\"Data\")\n",
    "        else:\n",
    "            legend_labels.append(f\"Total MC Stat. Unc. ({total_sum:.0f})\")\n",
    "            legend_labels.append(f\"Data ({total_data:.0f})\")\n",
    "            #legend_labels.append(f\"Data ({total_data:.0f})\")\n",
    "    ax_main.legend(legend_labels, loc='upper left', fontsize=8, frameon=False, ncol=3, bbox_to_anchor=(0.05, 0.98))\n",
    "\n",
    "    legend_labels_ratio = [\"y=1\", \"MC (Stat. Only)\", \"Data/MC\"]\n",
    "    ax_ratio.legend(legend_labels_ratio, loc='upper left', fontsize=6, frameon=False, ncol=3, bbox_to_anchor=(0.05, 0.98))\n",
    "\n",
    "\n",
    "    ax_main.text(0.00, 1.02, \"SBND \" + sample_str + \", Preliminary\",\n",
    "                 transform=ax_main.transAxes, fontsize=14, fontweight='bold')\n",
    "\n",
    "    #fig.savefig(\"./plots/pandora_df/2025_v10_06_00_05/data_vs_mc_tpc/\" + outname + \".pdf\", format='pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def draw_mc_data_shape_comp_per_slc(mc_bnb_cosmic_df, mc_low_th_df, data_offbeam_df, data_df, column, x_title, y_title, x_min, x_max, n_bins, out_name, is_logx = False, is_logy = False):\n",
    "    nuint_categ_col = ('gen', 'nuint_categ', '', '', '', '')\n",
    "\n",
    "    mc_bnb_cosmic_df_per_slc = mc_bnb_cosmic_df.groupby([('__ntuple'), ('entry'), ('rec.slc..index')])[[column, nuint_categ_col]].first()\n",
    "    mc_low_th_df_per_slc = mc_low_th_df.groupby([('__ntuple'), ('entry'), ('rec.slc..index')])[[column, nuint_categ_col]].first()\n",
    "    data_offbeam_df_per_slc = data_offbeam_df.groupby([('__ntuple'), ('entry'), ('rec.slc..index')])[[column, nuint_categ_col]].first()\n",
    "\n",
    "    data_df_per_slc = data_df.groupby([('__ntuple'), ('entry'), ('rec.slc..index')])[[column]].first()\n",
    "\n",
    "    mode_list_mc = [m for m in mode_list if ((m != -3) and (m != -4))]\n",
    "    var_mc_bnb_cosmic = [mc_bnb_cosmic_df_per_slc[mc_bnb_cosmic_df_per_slc[nuint_categ_col] == mode][column]for mode in mode_list_mc]\n",
    "    var_mc_low_th = [mc_low_th_df_per_slc[mc_low_th_df_per_slc.gen.nuint_categ == -4][column]]\n",
    "    var_data_offbeam = [data_offbeam_df_per_slc[data_offbeam_df_per_slc.gen.nuint_categ == -3][column]]\n",
    "    var_data = data_df_per_slc[column]\n",
    "    draw_reco_stacked_hist(var_mc_bnb_cosmic, var_mc_low_th, var_data_offbeam, is_logx, is_logy, x_title, y_title, x_min, x_max, n_bins, out_name, True, var_data)\n",
    "\n",
    "def draw_reco_valid_plots(mc_bnb_cosmic_df, mc_low_th_df, data_offbeam_df, data_df, suffix, is_logx = False, is_logy = False):\n",
    "    ## draw 1) clear cosmic, 2) nu score, 3) vertex x,y and z\n",
    "    \n",
    "    ## -- 1) Clear cosmic\n",
    "    clear_cosmic_col = ('slc', 'is_clear_cosmic', '', '', '', '')\n",
    "    draw_mc_data_shape_comp_per_slc(mc_bnb_cosmic_df, mc_low_th_df, data_offbeam_df, data_df, clear_cosmic_col, \"Is Clear Cosmic\", \"A.U.\", -0.5, 1.5, 2, suffix + \"_slc_is_clear_cosmic\", is_logx, is_logy)\n",
    "\n",
    "    ## -- 2) vertex x,y,z\n",
    "    vtx_x_col = ('slc', 'vertex', 'x', '', '', '')\n",
    "    vtx_y_col = ('slc', 'vertex', 'y', '', '', '')\n",
    "    vtx_z_col = ('slc', 'vertex', 'z', '', '', '')\n",
    "    draw_mc_data_shape_comp_per_slc(mc_bnb_cosmic_df, mc_low_th_df, data_offbeam_df, data_df, vtx_x_col, \"Slice Vertex X [cm]\", \"A.U.\", -300, 300, 60, suffix + \"_slc_vtx_x\", is_logx, is_logy)\n",
    "    draw_mc_data_shape_comp_per_slc(mc_bnb_cosmic_df, mc_low_th_df, data_offbeam_df, data_df, vtx_y_col, \"Slice Vertex Y [cm]\", \"A.U.\", -300, 300, 60, suffix + \"_slc_vtx_y\", is_logx, is_logy)\n",
    "    draw_mc_data_shape_comp_per_slc(mc_bnb_cosmic_df, mc_low_th_df, data_offbeam_df, data_df, vtx_z_col, \"Slice Vertex Z [cm]\", \"A.U.\", -100, 600, 70, suffix + \"_slc_vtx_z\", is_logx, is_logy)\n",
    "\n",
    "    mc_bnb_cosmic_df_east = mc_bnb_cosmic_df[mc_bnb_cosmic_df.slc.vertex.x < 0.]\n",
    "    mc_low_th_df_east = mc_low_th_df[mc_low_th_df.slc.vertex.x < 0.]\n",
    "    data_offbeam_df_east = data_offbeam_df[data_offbeam_df.slc.vertex.x < 0.]\n",
    "    data_df_east = data_df[data_df.slc.vertex.x < 0.]\n",
    "\n",
    "    mc_bnb_cosmic_df_west = mc_bnb_cosmic_df[mc_bnb_cosmic_df.slc.vertex.x > 0.]\n",
    "    mc_low_th_df_west = mc_low_th_df[mc_low_th_df.slc.vertex.x > 0.]\n",
    "    data_offbeam_df_west = data_offbeam_df[data_offbeam_df.slc.vertex.x > 0.]\n",
    "    data_df_west = data_df[data_df.slc.vertex.x > 0.]\n",
    "\n",
    "    draw_mc_data_shape_comp_per_slc(mc_bnb_cosmic_df_east, mc_low_th_df_east, data_offbeam_df_east, data_df_east, vtx_y_col, \"East TPC Slice Vertex Y [cm]\", \"A.U.\", -300, 300, 60, suffix + \"_slc_vtx_y_east\", is_logx, is_logy)\n",
    "    draw_mc_data_shape_comp_per_slc(mc_bnb_cosmic_df_west, mc_low_th_df_east, data_offbeam_df_east, data_df_west, vtx_y_col, \"West TPC Slice Vertex Y [cm]\", \"A.U.\", -300, 300, 60, suffix + \"_slc_vtx_y_west\", is_logx, is_logy)\n",
    "    \n",
    "    draw_mc_data_shape_comp_per_slc(mc_bnb_cosmic_df_east, mc_low_th_df_west, data_offbeam_df_west, data_df_east, vtx_z_col, \"East TPC Slice Vertex Z [cm]\", \"A.U.\", -100, 600, 70, suffix + \"_slc_vtx_z_east\", is_logx, is_logy)\n",
    "    draw_mc_data_shape_comp_per_slc(mc_bnb_cosmic_df_west, mc_low_th_df_west, data_offbeam_df_west, data_df_west, vtx_z_col, \"West TPC Slice Vertex Z [cm]\", \"A.U.\", -100, 600, 70, suffix + \"_slc_vtx_z_west\", is_logx, is_logy)\n",
    "\n",
    "    ## -- 3) Check longest and second longest track variables\n",
    "    longtrk_mc = mc_bnb_cosmic_df.sort_values(by=(\"pfp\", \"trk\", \"len\", \"\", \"\", \"\"), ascending=False).groupby(level=[0,1]).nth(0)\n",
    "    shorttrk_mc = mc_bnb_cosmic_df.sort_values(by=(\"pfp\", \"trk\", \"len\", \"\", \"\", \"\"), ascending=False).groupby(level=[0,1]).nth(1)\n",
    "\n",
    "    longtrk_mc_low_th = mc_low_th_df.sort_values(by=(\"pfp\", \"trk\", \"len\", \"\", \"\", \"\"), ascending=False).groupby(level=[0,1]).nth(0)\n",
    "    shorttrk_mc_low_th = mc_low_th_df.sort_values(by=(\"pfp\", \"trk\", \"len\", \"\", \"\", \"\"), ascending=False).groupby(level=[0,1]).nth(1)\n",
    "\n",
    "    longtrk_data_offbeam = data_offbeam_df.sort_values(by=(\"pfp\", \"trk\", \"len\", \"\", \"\", \"\"), ascending=False).groupby(level=[0,1]).nth(0)\n",
    "    shorttrk_data_offbeam = data_offbeam_df.sort_values(by=(\"pfp\", \"trk\", \"len\", \"\", \"\", \"\"), ascending=False).groupby(level=[0,1]).nth(1)\n",
    "\n",
    "    longtrk_data = data_df.sort_values(by=(\"pfp\", \"trk\", \"len\", \"\", \"\", \"\"), ascending=False).groupby(level=[0,1]).nth(0)\n",
    "    shorttrk_data = data_df.sort_values(by=(\"pfp\", \"trk\", \"len\", \"\", \"\", \"\"), ascending=False).groupby(level=[0,1]).nth(1)\n",
    "\n",
    "    trk_score_col = ('pfp', 'trackScore', '', '', '', '')\n",
    "    draw_mc_data_shape_comp_per_slc(longtrk_mc, longtrk_mc_low_th, longtrk_data_offbeam, longtrk_data, trk_score_col, \"Longest Track Track Score\", \"A.U.\", 0, 1., 50, suffix + \"_trk_score_long\", False, False)\n",
    "    draw_mc_data_shape_comp_per_slc(shorttrk_mc, shorttrk_mc_low_th, shorttrk_data_offbeam, shorttrk_data, trk_score_col, \"Second Longest Track Track Score\", \"A.U.\", 0, 1., 50, suffix + \"_trk_score_short\", False, False)\n",
    "\n",
    "    chi2_muon_col = ('pfp', 'trk', 'chi2pid', 'I2', 'chi2_muon', '')\n",
    "    chi2_proton_col = ('pfp', 'trk', 'chi2pid', 'I2', 'chi2_proton', '')\n",
    "    draw_mc_data_shape_comp_per_slc(longtrk_mc, longtrk_mc_low_th, longtrk_data_offbeam, longtrk_data, chi2_muon_col, r\"$\\mathrm{Longest~Track~\\chi^2_{\\mu}}$\", \"A.U.\", 0, 70., 50, suffix + \"_chi2pid_muon_long\", False, False)    \n",
    "    draw_mc_data_shape_comp_per_slc(longtrk_mc, longtrk_mc_low_th, longtrk_data_offbeam, longtrk_data, chi2_proton_col, r\"$\\mathrm{Longest~Track~\\chi^2_{p}}$\", \"A.U.\", 0, 400., 50, suffix + \"_chi2pid_proton_long\", False, False)\n",
    "\n",
    "    draw_mc_data_shape_comp_per_slc(shorttrk_mc, shorttrk_mc_low_th, shorttrk_data_offbeam, shorttrk_data, chi2_muon_col, r\"$\\mathrm{Second~Longest~Track~\\chi^2_{\\mu}}$\", \"A.U.\", 0, 70., 50, suffix + \"_chi2pid_muon_short\", False, False)\n",
    "    draw_mc_data_shape_comp_per_slc(shorttrk_mc, shorttrk_mc_low_th, shorttrk_data_offbeam, shorttrk_data, chi2_proton_col, r\"$\\mathrm{Second~Longest~Track~\\chi^2_{p}}$\", \"A.U.\", 0, 400., 50, suffix + \"_chi2pid_proton_short\", False, False)\n",
    "    \n",
    "    \n",
    "    trk_dir_x_col = ('pfp', 'trk', 'dir', 'x', '', '')\n",
    "    trk_dir_y_col = ('pfp', 'trk', 'dir', 'y', '', '')\n",
    "    trk_dir_z_col = ('pfp', 'trk', 'dir', 'z', '', '')\n",
    "\n",
    "    draw_mc_data_shape_comp_per_slc(longtrk_mc, longtrk_mc_low_th, longtrk_data_offbeam, longtrk_data, trk_dir_x_col, \"Longest Track Dir. X\", \"A.U.\", -1.2, 1.2, 60, suffix + \"_trk_dir_x_long\")\n",
    "    draw_mc_data_shape_comp_per_slc(longtrk_mc, longtrk_mc_low_th, longtrk_data_offbeam, longtrk_data, trk_dir_y_col, \"Longest Track Dir. Y\", \"A.U.\", -1.2, 1.2, 60, suffix + \"_trk_dir_y_long\")\n",
    "    draw_mc_data_shape_comp_per_slc(longtrk_mc, longtrk_mc_low_th, longtrk_data_offbeam, longtrk_data, trk_dir_z_col, \"Longest Track Dir. Z\", \"A.U.\", -1.2, 1.2, 60, suffix + \"_trk_dir_z_long\")\n",
    "    draw_mc_data_shape_comp_per_slc(longtrk_mc, longtrk_mc_low_th, longtrk_data_offbeam, longtrk_data, trk_dir_z_col, \"Longest Track Dir. Z\", \"A.U.\", -1.2, 1.2, 60, suffix + \"_trk_dir_z_long_logy\", False, True)\n",
    "\n",
    "\n",
    "    draw_mc_data_shape_comp_per_slc(shorttrk_mc, shorttrk_mc_low_th, shorttrk_data_offbeam, shorttrk_data, trk_dir_y_col, \"Second Longest Track Dir. Y\", \"A.U.\", -1.2, 1.2, 60, suffix + \"_trk_dir_y_short\")\n",
    "    draw_mc_data_shape_comp_per_slc(shorttrk_mc, shorttrk_mc_low_th, shorttrk_data_offbeam, shorttrk_data, trk_dir_z_col, \"Second Longest Track Dir. Z\", \"A.U.\", -1.2, 1.2, 60, suffix + \"_trk_dir_z_short_logy\", False, True)\n",
    "\n",
    "    trk_len_col = ('pfp', 'trk', 'len', '', '', '')\n",
    "    draw_mc_data_shape_comp_per_slc(longtrk_mc, longtrk_mc_low_th, longtrk_data_offbeam, longtrk_data, trk_len_col, \"Longest Track Length [cm]\", \"A.U.\", 0.5, 800., 80, suffix + \"_trk_len_long\", True, False)\n",
    "    draw_mc_data_shape_comp_per_slc(shorttrk_mc, shorttrk_mc_low_th, shorttrk_data_offbeam, shorttrk_data, trk_len_col, \"Second Longest Track Length [cm]\", \"A.U.\", 0.5, 800, 80, suffix + \"_trk_len_short\", True, False)\n",
    "\n",
    "    trk_start_x_col = ('pfp', 'trk', 'start', 'x', '', '')\n",
    "    trk_start_y_col = ('pfp', 'trk', 'start', 'y', '', '')\n",
    "    trk_start_z_col = ('pfp', 'trk', 'start', 'z', '', '')\n",
    "\n",
    "    trk_end_x_col = ('pfp', 'trk', 'end', 'x', '', '')\n",
    "    trk_end_y_col = ('pfp', 'trk', 'end', 'y', '', '')\n",
    "    trk_end_z_col = ('pfp', 'trk', 'end', 'z', '', '')\n",
    "    \n",
    "    draw_mc_data_shape_comp_per_slc(longtrk_mc, longtrk_mc_low_th, longtrk_data_offbeam, longtrk_data, trk_start_x_col, \"Longest Track Start X [cm]\", \"A.U.\", -300, 300, 120, suffix + \"_trk_start_x_long\", is_logx, is_logy)\n",
    "    draw_mc_data_shape_comp_per_slc(longtrk_mc, longtrk_mc_low_th, longtrk_data_offbeam, longtrk_data, trk_end_x_col, \"Longest Track End X [cm]\", \"A.U.\", -300, 300, 120, suffix + \"_trk_end_x_long\", is_logx, is_logy)\n",
    "    draw_mc_data_shape_comp_per_slc(shorttrk_mc, shorttrk_mc_low_th, shorttrk_data_offbeam, shorttrk_data, trk_start_x_col, \"Second Longest Track Start X [cm]\", \"A.U.\", -300, 300, 120, suffix + \"_trk_start_x_short\", is_logx, is_logy)\n",
    "    draw_mc_data_shape_comp_per_slc(shorttrk_mc, shorttrk_mc_low_th, shorttrk_data_offbeam, shorttrk_data, trk_end_x_col, \"Second Longest Track End X [cm]\", \"A.U.\", -300, 300, 120, suffix + \"_trk_end_x_short\", is_logx, is_logy)\n",
    "\n",
    "    draw_mc_data_shape_comp_per_slc(longtrk_mc, longtrk_mc_low_th, longtrk_data_offbeam, longtrk_data, trk_start_y_col, \"Longest Track Start Y [cm]\", \"A.U.\", -300, 300, 120, suffix + \"_trk_start_y_long\", is_logx, is_logy)\n",
    "    draw_mc_data_shape_comp_per_slc(longtrk_mc, longtrk_mc_low_th, longtrk_data_offbeam, longtrk_data, trk_end_y_col, \"Longest Track End Y [cm]\", \"A.U.\", -300, 300, 120, suffix + \"_trk_end_y_long\", is_logx, is_logy)\n",
    "    draw_mc_data_shape_comp_per_slc(shorttrk_mc, shorttrk_mc_low_th, shorttrk_data_offbeam, shorttrk_data, trk_start_y_col, \"Second Longest Track Start Y [cm]\", \"A.U.\", -300, 300, 120, suffix + \"_trk_start_y_short\", is_logx, is_logy)\n",
    "    draw_mc_data_shape_comp_per_slc(shorttrk_mc, shorttrk_mc_low_th, shorttrk_data_offbeam, shorttrk_data, trk_end_y_col, \"Second Longest Track End Y [cm]\", \"A.U.\", -300, 300, 120, suffix + \"_trk_end_y_short\", is_logx, is_logy)\n",
    "\n",
    "    draw_mc_data_shape_comp_per_slc(longtrk_mc, longtrk_mc_low_th, longtrk_data_offbeam, longtrk_data, trk_start_z_col, \"Longest Track Start Z [cm]\", \"A.U.\", -100, 600, 70, suffix + \"_trk_start_z_long\", is_logx, is_logy)\n",
    "    draw_mc_data_shape_comp_per_slc(longtrk_mc, longtrk_mc_low_th, longtrk_data_offbeam, longtrk_data, trk_end_z_col, \"Longest Track End Z [cm]\", \"A.U.\", -100, 600, 70, suffix + \"_trk_end_z_long\", is_logx, is_logy)\n",
    "    draw_mc_data_shape_comp_per_slc(shorttrk_mc, shorttrk_mc_low_th, shorttrk_data_offbeam, shorttrk_data, trk_start_z_col, \"Second Longest Track Start Z [cm]\", \"A.U.\", -100, 600, 70, suffix + \"_trk_start_z_short\", is_logx, is_logy)\n",
    "    draw_mc_data_shape_comp_per_slc(shorttrk_mc, shorttrk_mc_low_th, shorttrk_data_offbeam, shorttrk_data, trk_end_z_col, \"Second Longest Track End Z [cm]\", \"A.U.\", -100, 600, 70, suffix + \"_trk_end_z_short\", is_logx, is_logy)\n",
    "\n",
    "    #trk_x_ranges = [(0, 20), (20, 40), (40, 60), (60, 80), (80, 100), (100, 120), (120, 140), (140, 160), (160, 180), (180, 200)]\n",
    "    #for lo, hi in trk_x_ranges:\n",
    "    #    label = f\"{int(lo)}_{int(hi)}\"\n",
    "    #    shorttrk_mc_cut = shorttrk_mc[(shorttrk_mc[trk_start_x_col] >= lo) & (shorttrk_mc[trk_start_x_col] < hi)]\n",
    "    #    shorttrk_intime_mc_cut = shorttrk_intime_mc[(shorttrk_intime_mc[trk_start_x_col] >= lo) & (shorttrk_intime_mc[trk_start_x_col] < hi)]\n",
    "    #    shorttrk_data_cut = shorttrk_data[(shorttrk_data[trk_start_x_col] >= lo) & (shorttrk_data[trk_start_x_col] < hi)]\n",
    "\n",
    "    #    draw_mc_data_shape_comp_per_slc(shorttrk_mc_cut, shorttrk_intime_mc_cut, shorttrk_data_cut, chi2_muon_col, f\"Second Longest Track Chi2_mu (Track Start X: {lo}â€“{hi} cm)\", \"A.U.\", 0, 70., 35, suffix + f\"_chi2pid_muon_short_start_x_{label}\", is_logx, is_logy)\n",
    "\n",
    "\n",
    "\n",
    "    ## -- 4) more details for the longest track in slice\n",
    "    #trk_len_ranges = [(0, 10), (10, 20), (20, 40), (40, 80), (80, 120), (120, 200), (200, 300), (300, 500), (500, 1000)]\n",
    "\n",
    "    #for lo, hi in trk_len_ranges:\n",
    "    #    label = f\"{int(lo)}_{int(hi)}\"\n",
    "    \n",
    "    #    longtrk_mc_cut = longtrk_mc[(longtrk_mc[trk_len_col] >= lo) & (longtrk_mc[trk_len_col] < hi)]\n",
    "    #    longtrk_intime_mc_cut = longtrk_intime_mc[(longtrk_intime_mc[trk_len_col] >= lo) & (longtrk_intime_mc[trk_len_col] < hi)]\n",
    "    #    longtrk_data_cut = longtrk_data[(longtrk_data[trk_len_col] >= lo) & (longtrk_data[trk_len_col] < hi)]\n",
    "\n",
    "    #    draw_mc_data_shape_comp_per_slc(longtrk_mc_cut, longtrk_intime_mc_cut, longtrk_data_cut, trk_dir_x_col, f\"Longest Track Dir. X (Track Length: {lo}â€“{hi} cm)\", \"A.U.\", -1.2, 1.2, 60, suffix + f\"_trk_dir_x_long_len_{label}\", is_logx, is_logy)\n",
    "    #    draw_mc_data_shape_comp_per_slc(longtrk_mc_cut, longtrk_intime_mc_cut, longtrk_data_cut, trk_dir_y_col, f\"Longest Track Dir. Y (Track Length: {lo}â€“{hi} cm)\", \"A.U.\", -1.2, 1.2, 60, suffix + f\"_trk_dir_y_long_len_{label}\", is_logx, is_logy)\n",
    "    #    draw_mc_data_shape_comp_per_slc(longtrk_mc_cut, longtrk_intime_mc_cut, longtrk_data_cut, trk_dir_z_col, f\"Longest Track Dir. Z (Track Length: {lo}â€“{hi} cm)\", \"A.U.\", -1.2, 1.2, 60, suffix + f\"_trk_dir_z_long_len_{label}\", is_logx, is_logy)\n",
    "    #    draw_mc_data_shape_comp_per_slc(longtrk_mc_cut, longtrk_intime_mc_cut, longtrk_data_cut, trk_score_col, f\"Longest Track Track Score (Track Length: {lo}â€“{hi} cm)\", \"A.U.\", -1.2, 1.2, 60, suffix + f\"_trk_score_long_len_{label}\", is_logx, is_logy)\n",
    "\n",
    "\n",
    "\n",
    "### For drawing data blind plots: this function does not draw data points in bins with singal contribution greater than 10%\n",
    "def draw_reco_stacked_hist_blind(\n",
    "    var_mc, var_mc_low_th, var_offbeam_data, is_logx, is_logy,\n",
    "    title_x, title_y, x_min, x_max, nbins, outname,\n",
    "    data_overlay=False, var_data=[], draw_density=False,\n",
    "    blind=True, blind_req=0.1\n",
    "):\n",
    "    fig = plt.figure(figsize=(8, 8), dpi=100)\n",
    "    gs = gridspec.GridSpec(2, 1, height_ratios=[5, 1], hspace=0.10)\n",
    "    ax_main = fig.add_subplot(gs[0])\n",
    "    ax_ratio = fig.add_subplot(gs[1], sharex=ax_main)\n",
    "\n",
    "    if is_logx:\n",
    "        ax_main.set_xscale('log')\n",
    "        ax_ratio.set_xscale('log')\n",
    "    if is_logy:\n",
    "        ax_main.set_yscale('log')\n",
    "\n",
    "    ax_main.set_xlabel(\"\")\n",
    "    ax_main.set_ylabel(title_y)\n",
    "    ax_ratio.set_ylabel(\"Data/MC\", fontsize=12)\n",
    "    ax_ratio.set_xlabel(title_x, fontsize=12)\n",
    "    ax_ratio.axhline(1.0, color='red', linestyle='--', linewidth=1)\n",
    "    ax_ratio.set_ylim(0.4, 1.6)\n",
    "    ax_ratio.tick_params(width=2, length=6)\n",
    "    for spine in ax_ratio.spines.values():\n",
    "        spine.set_linewidth(2)\n",
    "\n",
    "    plt.setp(ax_main.get_xticklabels(), visible=False)\n",
    "\n",
    "    bins = np.logspace(np.log10(x_min), np.log10(x_max), nbins + 1) if is_logx else np.linspace(x_min, x_max, nbins + 1)\n",
    "    bin_centers = np.sqrt(bins[:-1] * bins[1:]) if is_logx else 0.5 * (bins[:-1] + bins[1:])\n",
    "\n",
    "    all_mc_data = var_mc + var_mc_low_th + var_offbeam_data\n",
    "    all_weights = (\n",
    "        [np.ones_like(data) * mc_pot_scale for data in var_mc] +\n",
    "        [np.ones_like(data) * mc_low_th_scale for data in var_mc_low_th] +\n",
    "        [np.ones_like(data) * intime_gate_scale for data in var_offbeam_data]\n",
    "    )\n",
    "\n",
    "    each_mc_hist_data = [\n",
    "        np.histogram(data, bins=bins, weights=w)[0]\n",
    "        for data, w in zip(all_mc_data, all_weights)\n",
    "    ]\n",
    "    total_mc = np.sum(each_mc_hist_data, axis=0)\n",
    "\n",
    "    hist_data, bins, _ = ax_main.hist(\n",
    "        all_mc_data, bins=bins, weights=all_weights,\n",
    "        stacked=True, color=colors, label=mode_labels,\n",
    "        edgecolor='none', linewidth=0, density=draw_density, histtype='stepfilled'\n",
    "    )\n",
    "\n",
    "    max_y = np.max(total_mc)\n",
    "    each_mc_hist_data = []\n",
    "    each_mc_hist_err2 = []  # sum of squared weights for error\n",
    "\n",
    "    for data, w in zip(all_mc_data, all_weights):\n",
    "        hist_vals, _ = np.histogram(data, bins=bins, weights=w)\n",
    "        hist_err2, _ = np.histogram(data, bins=bins, weights=np.square(w))\n",
    "        each_mc_hist_data.append(hist_vals)\n",
    "        each_mc_hist_err2.append(hist_err2)\n",
    "\n",
    "    total_mc = np.sum(each_mc_hist_data, axis=0)\n",
    "    total_mc_err2 = np.sum(each_mc_hist_err2, axis=0)\n",
    "    mc_stat_err = np.sqrt(total_mc_err2)\n",
    "    #mc_stat_err = np.sqrt(total_mc)\n",
    "\n",
    "    ax_main.bar(\n",
    "        bin_centers, 2 * mc_stat_err,\n",
    "        width=np.diff(bins), bottom=total_mc - mc_stat_err,\n",
    "        facecolor='none', edgecolor='black', hatch='xxxx',\n",
    "        linewidth=0.0, label='MC Stat. Unc.'\n",
    "    )\n",
    "\n",
    "    mc_stat_err_ratio = np.nan_to_num(mc_stat_err / total_mc, nan=0.)\n",
    "    mc_content_ratio = np.nan_to_num(total_mc / total_mc, nan=-999.)\n",
    "    ax_ratio.bar(\n",
    "        bin_centers, 2 * mc_stat_err_ratio,\n",
    "        width=np.diff(bins), bottom=mc_content_ratio - mc_stat_err_ratio,\n",
    "        facecolor='none', edgecolor='black', hatch='xxxx',\n",
    "        linewidth=0.0, label='MC Stat. Unc.'\n",
    "    )\n",
    "\n",
    "    if data_overlay:\n",
    "        ax_main.set_ylabel(\"A.U.\" if draw_density else f\"Events (POT = {target_pot:.2e})\")\n",
    "\n",
    "        counts, _ = np.histogram(var_data, bins=bins)\n",
    "        bin_widths = np.diff(bins)\n",
    "        total_data = np.sum(counts)\n",
    "\n",
    "        norm_counts = counts\n",
    "        data_eylow, data_eyhigh = sh.return_data_stat_err(counts)\n",
    "        if draw_density and total_data > 0:\n",
    "            norm_counts = counts / (total_data * bin_widths)\n",
    "            data_eylow = data_eylow / (total_data * bin_widths)\n",
    "            data_eyhigh = data_eyhigh / (total_data * bin_widths)\n",
    "        elif draw_density:\n",
    "            norm_counts[:] = 0\n",
    "            data_eylow[:] = 0\n",
    "            data_eyhigh[:] = 0\n",
    "\n",
    "        ### --> Apply blinding to bins where Signal fraction > blind_req\n",
    "        if blind:\n",
    "            signal_mc = each_mc_hist_data[0]  # index 0 = \"Signal\"\n",
    "            signal_frac = np.nan_to_num(signal_mc / total_mc, nan=0.)\n",
    "            blind_bins = signal_frac > blind_req\n",
    "            norm_counts[blind_bins] = -1\n",
    "            data_eylow[blind_bins] = 0\n",
    "            data_eyhigh[blind_bins] = 0\n",
    "\n",
    "        ax_main.errorbar(\n",
    "            bin_centers, norm_counts,\n",
    "            yerr=np.vstack((data_eylow, data_eyhigh)),\n",
    "            fmt='o', color='black', label='Data',\n",
    "            markersize=5, capsize=3, linewidth=1.5\n",
    "        )\n",
    "\n",
    "        max_y_data = np.max(norm_counts + data_eyhigh)\n",
    "        max_y = max(max_y, max_y_data)\n",
    "\n",
    "        data_ratio = np.nan_to_num(norm_counts / total_mc, nan=-1.)\n",
    "        data_ratio_eylow = np.nan_to_num(data_eylow / total_mc, nan=0.)\n",
    "        data_ratio_eyhigh = np.nan_to_num(data_eyhigh / total_mc, nan=0.)\n",
    "\n",
    "        ax_ratio.errorbar(\n",
    "            bin_centers, data_ratio,\n",
    "            yerr=np.vstack((data_ratio_eylow, data_ratio_eyhigh)),\n",
    "            fmt='o', color='black', label='Data',\n",
    "            markersize=5, capsize=3, linewidth=1.5\n",
    "        )\n",
    "\n",
    "    ax_main.set_xlim(x_min, x_max)\n",
    "    ax_main.set_ylim(0.1 if is_logy else 0.0, max_y * (600 if is_logy else 1.5))\n",
    "\n",
    "    accum_sum = [np.sum(data) for data in hist_data]\n",
    "    accum_sum = [0.] + accum_sum\n",
    "    total_sum = accum_sum[-1]\n",
    "    individual_sums = [accum_sum[i+1] - accum_sum[i] for i in range(len(accum_sum)-1)]\n",
    "    fractions = [(count / total_sum) * 100 for count in individual_sums]\n",
    "    legend_labels = [f\"{label} ({frac:.1f}%)\" for label, frac in zip(mode_labels[::-1], fractions[::-1])]\n",
    "\n",
    "    if data_overlay:\n",
    "        if draw_density:\n",
    "            legend_labels.append(\"Data\")\n",
    "        else:\n",
    "            legend_labels.append(f\"Total MC Stat. Unc. ({total_sum:.0f})\")\n",
    "            legend_labels.append(f\"Data ({total_data:.0f})\")\n",
    "\n",
    "    ax_main.legend(legend_labels, loc='upper left', fontsize=10, frameon=False, ncol=3, bbox_to_anchor=(0.05, 0.98))\n",
    "    ax_ratio.legend([\"y=1\", \"MC (Stat. Only)\", \"Data/MC\"], loc='upper left', fontsize=7, frameon=False, ncol=3, bbox_to_anchor=(0.05, 0.98))\n",
    "\n",
    "    ax_main.text(0.00, 1.02, \"SBND \" + sample_str + \", Preliminary\",\n",
    "                 transform=ax_main.transAxes, fontsize=14, fontweight='bold')\n",
    "\n",
    "    #fig.savefig(f\"./plots/pandora_df/2025_v10_06_00_05/data_vs_mc_tpc/{outname}.pdf\", format='pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def draw_mc_data_shape_comp_per_slc_blind(mc_bnb_cosmic_df, mc_low_th_df, data_offbeam_df, data_df, column, x_title, y_title, x_min, x_max, n_bins, out_name, is_logx = False, is_logy = False, blind = True, blind_req = 0.1):\n",
    "    nuint_categ_col = ('gen', 'nuint_categ', '', '', '', '')\n",
    "\n",
    "    mc_bnb_cosmic_df_per_slc = mc_bnb_cosmic_df.groupby([('__ntuple'), ('entry'), ('rec.slc..index')])[[column, nuint_categ_col]].first()\n",
    "    mc_low_th_df_per_slc = mc_low_th_df.groupby([('__ntuple'), ('entry'), ('rec.slc..index')])[[column, nuint_categ_col]].first()\n",
    "    data_offbeam_df_per_slc = data_offbeam_df.groupby([('__ntuple'), ('entry'), ('rec.slc..index')])[[column, nuint_categ_col]].first()\n",
    "    data_df_per_slc = data_df.groupby([('__ntuple'), ('entry'), ('rec.slc..index')])[[column]].first()\n",
    "\n",
    "    mode_list_mc = [m for m in mode_list if ((m != -3) and (m != -4))]\n",
    "    var_mc_bnb_cosmic = [mc_bnb_cosmic_df_per_slc[mc_bnb_cosmic_df_per_slc[nuint_categ_col] == mode][column]for mode in mode_list_mc]\n",
    "    var_mc_low_th = [mc_low_th_df_per_slc[mc_low_th_df_per_slc.gen.nuint_categ == -4][column]]\n",
    "    var_data_offbeam = [data_offbeam_df_per_slc[data_offbeam_df_per_slc.gen.nuint_categ == -3][column]]\n",
    "    var_data = data_df_per_slc[column]\n",
    "\n",
    "    draw_reco_stacked_hist_blind(var_mc_bnb_cosmic, var_mc_low_th, var_data_offbeam, is_logx, is_logy, x_title, y_title, x_min, x_max, n_bins, out_name, True, var_data, False, blind, blind_req)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform evt selections and plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### No cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_reco_valid_plots(mc_bnb_cosmic_dfs[\"evt\"], mc_rockbox_th1to100_dfs[\"evt\"], data_offbeam_light_dfs[\"evt\"], data_bnb_light_dfs[\"evt\"], \"a_a_no_cut\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### PE cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_bnb_cosmic_dfs[\"evt\"] = mc_bnb_cosmic_dfs[\"evt\"][mc_bnb_cosmic_dfs[\"evt\"].pemask]\n",
    "mc_rockbox_th1to100_dfs[\"evt\"] = mc_rockbox_th1to100_dfs[\"evt\"][mc_rockbox_th1to100_dfs[\"evt\"].pemask]\n",
    "data_offbeam_light_dfs[\"evt\"] = data_offbeam_light_dfs[\"evt\"][data_offbeam_light_dfs[\"evt\"].pemask]\n",
    "data_bnb_light_dfs[\"evt\"] = data_bnb_light_dfs[\"evt\"][data_bnb_light_dfs[\"evt\"].pemask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_reco_valid_plots(mc_bnb_cosmic_dfs[\"evt\"], mc_rockbox_th1to100_dfs[\"evt\"], data_offbeam_light_dfs[\"evt\"], data_bnb_light_dfs[\"evt\"], \"a_a_b_pe_cut\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Vtx in FV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_bnb_cosmic_dfs[\"evt\"] = mc_bnb_cosmic_dfs[\"evt\"][InFV_nohiyz(mc_bnb_cosmic_dfs[\"evt\"].slc.vertex)]\n",
    "mc_rockbox_th1to100_dfs[\"evt\"] = mc_rockbox_th1to100_dfs[\"evt\"][InFV_nohiyz(mc_rockbox_th1to100_dfs[\"evt\"].slc.vertex)]\n",
    "data_offbeam_light_dfs[\"evt\"] = data_offbeam_light_dfs[\"evt\"][InFV_nohiyz(data_offbeam_light_dfs[\"evt\"].slc.vertex)]\n",
    "data_bnb_light_dfs[\"evt\"] = data_bnb_light_dfs[\"evt\"][InFV_nohiyz(data_bnb_light_dfs[\"evt\"].slc.vertex)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_reco_valid_plots(mc_bnb_cosmic_dfs[\"evt\"], mc_rockbox_th1to100_dfs[\"evt\"], data_offbeam_light_dfs[\"evt\"], data_bnb_light_dfs[\"evt\"], \"a_b_vtxfvcut\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Not clear cosmic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_bnb_cosmic_dfs[\"evt\"] = mc_bnb_cosmic_dfs[\"evt\"][mc_bnb_cosmic_dfs[\"evt\"].slc.is_clear_cosmic == 0]\n",
    "mc_rockbox_th1to100_dfs[\"evt\"] = mc_rockbox_th1to100_dfs[\"evt\"][mc_rockbox_th1to100_dfs[\"evt\"].slc.is_clear_cosmic == 0]\n",
    "data_offbeam_light_dfs[\"evt\"] = data_offbeam_light_dfs[\"evt\"][data_offbeam_light_dfs[\"evt\"].slc.is_clear_cosmic == 0]\n",
    "data_bnb_light_dfs[\"evt\"] = data_bnb_light_dfs[\"evt\"][data_bnb_light_dfs[\"evt\"].slc.is_clear_cosmic == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_reco_valid_plots(mc_bnb_cosmic_dfs[\"evt\"], mc_rockbox_th1to100_dfs[\"evt\"], data_offbeam_light_dfs[\"evt\"], data_bnb_light_dfs[\"evt\"], \"a_vtx_fv_and_not_clear_cosmic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_py39_cafpyana",
   "language": "python",
   "name": "venv_py39_cafpyana"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
